{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Down the process of Predictive Modeling\n",
    "I always focus on investing quality time during initial phase of model building like hypothesis generation / brain storming session(s) / discussion(s) or understanding the domain. All these activities help me to relate to the problem, which eventually leads me to design more powerful business solutions. There are good reasons why you should spend this time up front:\n",
    "\n",
    "You have enough time to invest and you are fresh ( It has an impact)\n",
    "You are not biased with other data points or thoughts (I always suggest, do hypothesis generation before deep diving in data)\n",
    "At later stage, you would be in a hurry to complete the project and not able to spend quality time\n",
    "This stage will need a quality time so I am not mentioning the timeline here, I would recommend you to make this as a standard practice. It will help you to build a better predictive models and result in less iteration of work at later stages. Let’s look at the remaining stages in first model build with timelines:\n",
    "\n",
    "Descriptive analysis on the Data – 50% time\n",
    "Data treatment (Missing value and outlier fixing) – 40% time\n",
    "Data Modelling – 4% time\n",
    "Estimation of performance – 6% time\n",
    "P.S. This is the split of time spent only for the first model build\n",
    "\n",
    "Let’s go through the process step by step (with estimates of time spent in each step):\n",
    "\n",
    " \n",
    "\n",
    "## Stage 1: Descriptive Analysis / Data Exploration:\n",
    "In my initial days as data scientist, data exploration used to take a lot of time for me. With time, I have automated a lot of operations on the data. Given that data prep takes up 50% of the work in building a first model, the benefits of automation are obvious. You can look at “7 Steps of data exploration” to look at the most common operations of data exploration.\n",
    "\n",
    "Tavish has already mentioned in his article that with advanced machine learning tools coming in race, time taken to perform this task has been significantly reduced. Since this is our first benchmark model, we do away with any kind of feature engineering. Hence, the time you might need to do descriptive analysis is restricted to know missing values and big features which are directly visible. In my methodology, you will need 2 minutes to complete this step (Assumption, 100,000 observations in data set).\n",
    "\n",
    "The operations I perform for my first model include:\n",
    "\n",
    "Identify ID, Input and Target features\n",
    "Identify categorical and numerical features\n",
    "Identify columns with missing values\n",
    " \n",
    "\n",
    "## Stage 2: Data Treatment (Missing values treatment):\n",
    "There are various ways to deal with it. For our first model, we will focus on the smart and quick techniques to build your first effective model (These are already discussed by Tavish in his article, I am adding a few methods)\n",
    "\n",
    "Create dummy flags for missing value(s) : It works, sometimes missing values itself carry a good amount of information.\n",
    "Impute missing value with mean/ median/ any other easiest method : Mean and Median imputation performs well, mostly people prefer to impute with mean value but in case of skewed distribution I would suggest you to go with median. Other Intelligent methods are imputing values by similar case mean and median imputation using other relevant features or building a model. For Example: In Titanic survival challenge, you can impute missing values of Age using salutation of passengers name Like “Mr.”, “Miss.”,”Mrs.”,”Master” and others and this has shown good impact on model performance.\n",
    "Impute missing value of categorical variable: Create a new level to impute categorical variable so that all missing value is coded as a single value say “New_Cat” or you can look at the frequency mix and impute the missing value with value having higher frequency.\n",
    "With such simple methods of data treatment, you can reduce the time to treat data to 3-4 minutes.\n",
    "\n",
    " \n",
    "\n",
    "## Stage 3. Data Modelling :\n",
    "I recommend to use any one of GBM / Random Forest techniques, depending on the business problem. These two techniques are extremely effective to create a benchmark solution. I have seen data scientist are using these two methods often as their first model and in some cases it acts as a final model also. This will take maximum amount of time (~4-5 minutes).\n",
    "\n",
    " \n",
    "\n",
    "## Stage 4. Estimation of Performance:\n",
    "There are various methods to validate your model performance, I would suggest you to divide your train data set into Train and validate (ideally 70:30) and build model based on 70% of train data set. Now, cross-validate it using 30% of validate data set and evaluate the performance using evaluation metric. This finally takes 1-2 minutes to execute and document.\n",
    "\n",
    "Intent of this article is not to win the competition, but to establish a benchmark for our self. Let’s look at the python codes to perform above steps and build your first model with higher impact.\n",
    "\n",
    " \n",
    "\n",
    "Let’s start putting this into action\n",
    "I have assumed you have done all the hypothesis generation first and you are good with basic data science using python.  I am illustrating this with an example of data science challenge. Let’s look at the structure:\n",
    "\n",
    "### Step 1 : Import required libraries and read test and train data set. Append both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train=pd.read_csv('C:/Users/Analytics Vidhya/Desktop/challenge/Train.csv')\n",
    "test=pd.read_csv('C:/Users/Analytics Vidhya/Desktop/challenge/Test.csv')\n",
    "train['Type']='Train' #Create a flag for Train and Test Data set\n",
    "test['Type']='Test'\n",
    "fullData = pd.concat([train,test],axis=0) #Combined both Train and Test Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Step 2 of the framework is not required in Python. On to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: View the column names / summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData.columns # This will show all the column names\n",
    "fullData.head(10) # Show first 10 records of dataframe\n",
    "fullData.describe() #You can look at summary of numerical fields by using describe() function\n",
    "\n",
    "Capture_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Identify the a) ID variables b)  Target variables c) Categorical Variables d) Numerical Variables e) Other Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_col = ['REF_NO']\n",
    "target_col = [\"Account.Status\"]\n",
    "cat_cols = ['children','age_band','status','occupation','occupation_partner','home_status','family_income','self_employed', 'self_employed_partner','year_last_moved','TVarea','post_code','post_area','gender','region']\n",
    "num_cols= list(set(list(fullData.columns))-set(cat_cols)-set(ID_col)-set(target_col)-set(data_col))\n",
    "other_col=['Type'] #Test and Train Data set identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Identify the variables with missing values and create a flag for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData.isnull().any()#Will return the feature with True or False,True means have missing value else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat_cols = num_cols+cat_cols # Combined numerical and Categorical variables\n",
    "\n",
    "#Create a new variable for each variable having missing value with VariableName_NA \n",
    "# and flag missing value with 1 and other with 0\n",
    "\n",
    "for var in num_cat_cols:\n",
    "    if fullData[var].isnull().any()==True:\n",
    "        fullData[var+'_NA']=fullData[var].isnull()*1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Impute Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute numerical missing values with mean\n",
    "fullData[num_cols] = fullData[num_cols].fillna(fullData[num_cols].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute categorical missing values with -9999\n",
    "fullData[cat_cols] = fullData[cat_cols].fillna(value = -9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 : Create a label encoders for categorical variables and split the data set to train & test, further split the train data set to Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create label encoders for categorical features\n",
    "for var in cat_cols:\n",
    "    number = LabelEncoder() # label Encounter used to normalize labels\n",
    "    fullData[var] = number.fit_transform(fullData[var].astype('str'))\n",
    "\n",
    "#Target variable is also a categorical so convert it\n",
    "fullData[\"Account.Status\"] = number.fit_transform(fullData[\"Account.Status\"].astype('str'))\n",
    "\n",
    "train=fullData[fullData['Type']=='Train']\n",
    "test=fullData[fullData['Type']=='Test']\n",
    "\n",
    "train['is_train'] = np.random.uniform(0, 1, len(train)) <= .75\n",
    "Train, Validate = train[train['is_train']==True], train[train['is_train']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 : Pass the imputed and dummy (missing values flags) variables into the modelling process. I am using random forest to predict the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=list(set(list(fullData.columns))-set(ID_col)-set(target_col)-set(other_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Train[list(features)].values\n",
    "y_train = Train[\"Account.Status\"].values\n",
    "x_validate = Validate[list(features)].values\n",
    "y_validate = Validate[\"Account.Status\"].values\n",
    "x_test=test[list(features)].values\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "rf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 : Check performance and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = rf.predict_proba(x_validate)\n",
    "fpr, tpr, _ = roc_curve(y_validate, status[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print roc_auc\n",
    "\n",
    "final_status = rf.predict_proba(x_test)\n",
    "test[\"Account.Status\"]=final_status[:,1]\n",
    "test.to_csv('C:/Users/Analytics Vidhya/Desktop/model_output.csv',columns=['REF_NO','Account.Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
